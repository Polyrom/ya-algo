"""
-- ПРИНЦИП РАБОТЫ --
Т.к. по условию задачи для рассчета релевантности текста нас не интересует положение слова в тексте,
а только их общее количество, на подготовительном этапе строится индекс всех слов, встречающихся вo всех текстах.
Индекс предствляет собой хэш-таблицу, где по ключу находится слово, а по значению - вложенная хэш-таблица
с ключом - порядковым числом текста (от 1) и значением - общим количеством данного слова в тексте.

Затем для каждого поискового запроса проверяется наличие слов в индексе, и общая релевантность текста сохраняется
в промежуточную хэш-таблицу с ключом-порядковым номером текста и значением-суммой "весов".
Ключи промежуточной таблицы преобразуются в отсортированный массив, (неубывающий по "весу" и неприбывающий по
порядковому номеру текста).


-- ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ --
Предварительное построение индекса.
Для каждого текста строится индекс (словарь {word: count}), который гарантированно правильно подсчитывает вхождения слов.
При добавлении в индекс для каждого слова записывается порядковый номер текста с индексированием от 1
и соответствующее количество вхождений.
Поскольку Counter и словарь defaultdict(dict) в Python гарантируют корректность подсчёта, индекс строится верно.

Алгоритм подсчета релевантности текстов.
Запрос разбивается на уникальные слова, чтобы избежать дублирования.
Для каждого уникального слова из запроса:
- Если слово есть в индексе, берётся {порядковый номер текста: количество вхождений слова}.
- Для каждого текста к его релевантности прибавляется количество вхождений.
Таким образом, релевантность документа — это сумма вхождений всех слов запроса в этот документ, что соответствует условию задачи.

После подсчёта релевантности документы сортируются по убыванию (-score), а при равенстве — по возрастанию doc_id.
Это соответствует условию задачи.

-- ВРЕМЕННАЯ СЛОЖНОСТЬ --
- Предварительный этап. Построение индекса {словo: {номер текста: вес слова в тексте}} -
1. Обход n текстов: O(n), n <= 10^4.
2. Для каждого текста:
   2.1. Разбиение на слова:
       - O(m), m <= 1000 символов
       - Создает список из k слов, где k <= m
   2.2. Подсчет частот:
       - Для каждого слова:
         * Вычисление хеша: O(l), l - длина самого длинного слова
         * Вставка/обновление в хэш-таблицу: O(1) в среднем
       - Итого: O(k * l)
   2.3. Запись в основной индекс:
       - Для каждого из u уникальных слов:
         * Вычисление хеша: O(l)
         * Вставка в defaultdict: O(1)
       - Итого: O(u * l)

Можно упростить исходя из условий задачи:
    - k <= m (слов не больше, чем символов)
    - u <= k (уникальных слов не больше, чем слов)
    - l <= m (длина слова <= длины текста)
Т.е. O(k*l) и O(u*l) доминируют над O(m).

Итоговая сложность этапа: O(n * k * l).

-- Основной этап. Поиск топ-5 релевантных документов --
Этап выполняется для каждого поискового запроса (m).
1. Подготовка запроса:
   1.1. Разбиение запроса на слова: O(s), s <= 100 символов
   1.2. Создание множества уникальных слов: O(w), w <= 100
2. Для каждого уникального слова:
   2.1. Поиск слова в индексе: O(1) в среднем (хеш-таблица)
   2.2. Если слово найдено:
       - Обход документов с этим словом: O(d), d <= 10^4 (в худшем случае)
       - Обновление релевантности: O(1) на документ
3. Сортировка результатов:
   3.1. Создание списка пар: O(r), r <= n (число релевантных документов)
   3.2. Сортировка: O(r log r)
   3.3. Выбор топ-5: O(1)

Можно упростить исходя из условий задачи:
    - s <= 100, O(s) пренебрежимо мало
    - w <= 100 (в худшем случае поисковый запрос состоит из одного "слова" длиной 100 символов)
    - d <= 10^4 (в худшем случае слово есть во всех документах)
    - r <= n <= 10^4

Итоговая сложность этапа: O(w * d + r log r)

Итого, общая временная сложность обоих этапов решения составляет:
O(n * k * l) + O(m * (w * d + r log r)) ≈ O(n + m * d_avg), где n - число документов (<=10^4), m - количество поисковых
запросов (<=10^4), d_avg - среднее число документов на слово.

-- ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ --
Дополнительная память выделяется под:
1. Индекс слов во всех документах.
Для каждого уникального слова в корпусе текстов храним:
    - Само слово: O(v * l_avg), где v - количество уникальных слов во всех документах и l_avg - средняя длина слова в корпусе.
    - Словарь соответствий: O(v * d_avg), где d_avg - среднее число документов на слово.

2. Промежуточные структуры данных при поиске топ-5 релевантных слов.
Множество поисковых слов: O(w * l_q), где w <= 100, a l_q - средняя длина слова в запросе.
Словарь relevance_weights: O(r) r <= n (число релевантных документов)
Список scored_docs: O(r)

Общая пространственная сложность:
O((v * l_avg) + (v * d_avg) + (w * l_q) + 2r) ≈ O(v × d_avg), т.к. v * d_avg доминирует над v * l_avg.
"""

from collections import Counter, defaultdict


def build_text_indices(texts: list[str]) -> dict[str, dict[int, int]]:
    text_indices = defaultdict(dict)
    for i, text in enumerate(texts, start=1):
        text_index = Counter(text.strip().split())
        for word, score in text_index.items():
            text_indices[word][i] = score
    return text_indices


def find_relevant_texts(indices: dict[str, dict[int, int]], search: str) -> list[int]:
    search_tokens = set(search.strip().split())
    relevance_weights = defaultdict(int)
    for st in search_tokens:
        if texts_word_weight := indices.get(st):
            for text_id, score in texts_word_weight.items():
                relevance_weights[text_id] += score
    scored_docs = [(-score, text_id) for text_id, score in relevance_weights.items()]
    scored_docs.sort()
    return [doc_id for _, doc_id in scored_docs[:5]]


def read_input() -> tuple[list[str], list[str]]:
    n = int(input())
    texts = [input().strip() for _ in range(n)]
    searches = []
    n = int(input())
    searches = [input().strip() for _ in range(n)]
    return texts, searches


if __name__ == "__main__":
    texts, searches = read_input()
    indices = build_text_indices(texts)
    for s in searches:
        print(" ".join(list(map(str, find_relevant_texts(indices, s)))))
